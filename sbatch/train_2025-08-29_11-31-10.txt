wandb: Currently logged in as: klyczek (bozek-lab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.1
wandb: Run data is saved locally in /projects/ag-bozek/klyczek/wandb/run-20250829_113114-mbxqrdqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-snow-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/bozek-lab/zebrafish
wandb: üöÄ View run at https://wandb.ai/bozek-lab/zebrafish/runs/mbxqrdqg
/projects/ag-bozek/klyczek/utils.py:69: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
  plt.legend()
wandb: uploading fish_bout_transformer.pth
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: train_loss ‚ñÅ‚ñÅ‚ñÉ‚ñà‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñÅ
wandb:   val_loss ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÑ
wandb: 
wandb: Run summary:
wandb:      epoch 51
wandb: train_loss 1.58383
wandb:   val_loss 9.37609
wandb: 
wandb: üöÄ View run snowy-snow-98 at: https://wandb.ai/bozek-lab/zebrafish/runs/mbxqrdqg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/bozek-lab/zebrafish
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20250829_113114-mbxqrdqg/logs
{'model': {'num_layers': 4, 'd_model': 16, 'dim_feedforward': 64, 'dropout': 0, 'nhead': 4}, 'training': {'loss': 'mse', 'train_fraction': 0.7, 'val_fraction': 0.15, 'early_stopping': {'enabled': True, 'patience': 20, 'delta': '1e-3'}, 'batch_size': 1, 'lr': '1e-3', 'epochs': 5000, 'optimizer': 'adam'}, 'masking': {'type': 'random', 'percentage': 0.1}, 'data': {'conditions_path': 'sensory_contexts_data.npy', 'processed_dir': 'processed_data', 'input_dim': 20, 'sequence_length': 9, 'raw_path': 'Datasets/JM_data/pool_ex8_PCs.h5'}, 'seed': 42}
Using device: cuda
Total fish: 463, PCA shape: (463, 11651, 20), Nonzero mask shape: (463, 11651)
Conditions index shape: (463,)
Train size: 324, Val+Test size: 139
Validation size: 69, Test size: 70
Validation: [136 196 304  69 382  42 340 194 172 375 139 257 294  18 433  39 327   3
  64 175 162  97  55 190   0  73 460 420 189 315  90 402 211 451 323 431
 287 445 128 400 339   9  70 225 380 260  48  60 394 215 258 143  92 275
 123  36 351 388 286 459 229 145 120 203 328 218 406 115 306], Test: [ 14 243 280 343 295 336  46  37  95 256 147  34 372 164 105 341 154  76
 171 193 408 246 165 414   8 305 106 373  11  30 335 111 198  85 387 319
  51  63 245  57 363 163 357 170 457 224 418 269 118 446 401 422 272 192
 296 236 409 126 132 251  75 432 298 322 398 312 220 290 151 338]
Filtered PCA shapes before sliding windowing: train=855666, val=168579, test=179164
------------------------------------------------
pca_train (855666, 20)
pca_val (168579, 20)
pca_test (179164, 20)
Creating sliding windows of size 9...
Creating sliding windows of size 9...
Creating sliding windows of size 9...
Sliding window shapes: train=(855658, 9, 20), val=(168571, 9, 20), test=(179156, 9, 20)
used for normalization Mean: -0.020540177822113037, Std: 2.958566427230835
computing statistics of normalized data...
Computing statistics...
data shape (855666, 20)
Mean: [-0.23200016 -0.09907612 -0.00295992 -0.06568918  0.04590619  0.00567815
 -0.03474114  0.03558569 -0.03564373  0.01902646  0.00099173 -0.05185678
  0.0052082   0.01394337 -0.01173412 -0.01064747  0.01432809 -0.00253002
 -0.00335769 -0.00123489]
Std: [10.038458    5.496974    3.897122    2.8285718   2.2303479   1.8786765
  1.4661893   1.2725794   1.1405882   1.0457467   0.9947754   0.89248496
  0.8747207   0.8315738   0.7977842   0.7272971   0.67204624  0.65969485
  0.61814487  0.585693  ]
Min: [-43.64716  -21.541899 -23.157005 -31.334883 -25.56383  -21.92232
 -24.143694 -22.363401 -26.16261  -16.465778 -23.70673  -18.04575
 -15.623499 -12.573859 -15.557911 -19.924786 -12.032966 -17.378014
 -14.739848 -12.859519]
Max: [41.182434 24.70979  26.262136 31.009045 26.294428 22.97691  28.348349
 20.850065 23.674097 17.012926 22.952692 21.350965 11.419177 13.425751
 21.698322 18.035275 14.125225 18.658062 19.302763 11.227902]
Train data shape: torch.Size([1, 9, 20])
Using learning rate: 0.001
FishBoutEncoder(
  (input_proj): Linear(in_features=20, out_features=16, bias=True)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-3): 4 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=64, bias=True)
        (dropout): Dropout(p=0, inplace=False)
        (linear2): Linear(in_features=64, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0, inplace=False)
        (dropout2): Dropout(p=0, inplace=False)
      )
    )
  )
  (output): Linear(in_features=16, out_features=20, bias=True)
)
Starting training...
Epoch 1 | Train Loss: 1.9650 | Val Loss: 9.3615
Validation loss improved. Model saved to best_model_0.pth
Epoch 2 | Train Loss: 5.9319 | Val Loss: 9.3273
Validation loss improved. Model saved to best_model_1.pth
Epoch 3 | Train Loss: 1.7131 | Val Loss: 9.3628
No improvement. Early stopping patience 1/20
Epoch 4 | Train Loss: 5.5963 | Val Loss: 9.3519
No improvement. Early stopping patience 2/20
Epoch 5 | Train Loss: 13.9520 | Val Loss: 9.3614
No improvement. Early stopping patience 3/20
Epoch 6 | Train Loss: 13.7710 | Val Loss: 9.3718
No improvement. Early stopping patience 4/20
Epoch 7 | Train Loss: 6.4596 | Val Loss: 9.3376
No improvement. Early stopping patience 5/20
Epoch 8 | Train Loss: 6.3718 | Val Loss: 9.3662
No improvement. Early stopping patience 6/20
Epoch 9 | Train Loss: 3.1975 | Val Loss: 9.3697
No improvement. Early stopping patience 7/20
Epoch 10 | Train Loss: 4.9928 | Val Loss: 9.3793
No improvement. Early stopping patience 8/20
Epoch 11 | Train Loss: 6.1525 | Val Loss: 9.3879
No improvement. Early stopping patience 9/20
Epoch 12 | Train Loss: 2.7174 | Val Loss: 9.3719
No improvement. Early stopping patience 10/20
Epoch 13 | Train Loss: 3.3445 | Val Loss: 9.3707
No improvement. Early stopping patience 11/20
Epoch 14 | Train Loss: 3.3591 | Val Loss: 9.3401
No improvement. Early stopping patience 12/20
Epoch 15 | Train Loss: 5.9698 | Val Loss: 9.3701
No improvement. Early stopping patience 13/20
Epoch 16 | Train Loss: 4.6774 | Val Loss: 9.3604
No improvement. Early stopping patience 14/20
Epoch 17 | Train Loss: 2.6351 | Val Loss: 9.3504
No improvement. Early stopping patience 15/20
Epoch 18 | Train Loss: 3.3887 | Val Loss: 9.3891
No improvement. Early stopping patience 16/20
Epoch 19 | Train Loss: 4.5811 | Val Loss: 9.3601
No improvement. Early stopping patience 17/20
Epoch 20 | Train Loss: 1.6332 | Val Loss: 9.3202
Validation loss improved. Model saved to best_model_19.pth
Epoch 21 | Train Loss: 4.3590 | Val Loss: 9.3520
No improvement. Early stopping patience 1/20
Epoch 22 | Train Loss: 3.3486 | Val Loss: 9.4164
No improvement. Early stopping patience 2/20
Epoch 23 | Train Loss: 1.5717 | Val Loss: 9.3705
No improvement. Early stopping patience 3/20
Epoch 24 | Train Loss: 4.5148 | Val Loss: 9.3834
No improvement. Early stopping patience 4/20
Epoch 25 | Train Loss: 2.4754 | Val Loss: 9.3339
No improvement. Early stopping patience 5/20
Epoch 26 | Train Loss: 1.4950 | Val Loss: 9.3744
No improvement. Early stopping patience 6/20
Epoch 27 | Train Loss: 4.2610 | Val Loss: 9.3622
No improvement. Early stopping patience 7/20
Epoch 28 | Train Loss: 2.3979 | Val Loss: 9.3273
No improvement. Early stopping patience 8/20
Epoch 29 | Train Loss: 1.5623 | Val Loss: 9.3776
No improvement. Early stopping patience 9/20
Epoch 30 | Train Loss: 4.1555 | Val Loss: 9.3571
No improvement. Early stopping patience 10/20
Epoch 31 | Train Loss: 4.6026 | Val Loss: 9.3107
Validation loss improved. Model saved to best_model_30.pth
Epoch 32 | Train Loss: 13.8411 | Val Loss: 9.3401
No improvement. Early stopping patience 1/20
Epoch 33 | Train Loss: 3.9984 | Val Loss: 9.3523
No improvement. Early stopping patience 2/20
Epoch 34 | Train Loss: 1.5022 | Val Loss: 9.3144
No improvement. Early stopping patience 3/20
Epoch 35 | Train Loss: 3.8811 | Val Loss: 9.3618
No improvement. Early stopping patience 4/20
Epoch 36 | Train Loss: 3.8073 | Val Loss: 9.3706
No improvement. Early stopping patience 5/20
Epoch 37 | Train Loss: 3.0550 | Val Loss: 9.3960
No improvement. Early stopping patience 6/20
Epoch 38 | Train Loss: 1.4991 | Val Loss: 9.3868
No improvement. Early stopping patience 7/20
Epoch 39 | Train Loss: 6.0448 | Val Loss: 9.3181
No improvement. Early stopping patience 8/20
Epoch 40 | Train Loss: 1.4543 | Val Loss: 9.3655
No improvement. Early stopping patience 9/20
Epoch 41 | Train Loss: 4.7227 | Val Loss: 9.3752
No improvement. Early stopping patience 10/20
Epoch 42 | Train Loss: 2.4489 | Val Loss: 9.3691
No improvement. Early stopping patience 11/20
Epoch 43 | Train Loss: 3.0135 | Val Loss: 9.3729
No improvement. Early stopping patience 12/20
Epoch 44 | Train Loss: 4.7001 | Val Loss: 9.3372
No improvement. Early stopping patience 13/20
Epoch 45 | Train Loss: 22.8026 | Val Loss: 9.3938
No improvement. Early stopping patience 14/20
Epoch 46 | Train Loss: 1.3451 | Val Loss: 9.3828
No improvement. Early stopping patience 15/20
Epoch 47 | Train Loss: 2.4142 | Val Loss: 9.4063
No improvement. Early stopping patience 16/20
Epoch 48 | Train Loss: 4.6317 | Val Loss: 9.3823
No improvement. Early stopping patience 17/20
Epoch 49 | Train Loss: 13.8625 | Val Loss: 9.3456
No improvement. Early stopping patience 18/20
Epoch 50 | Train Loss: 22.3046 | Val Loss: 9.3624
No improvement. Early stopping patience 19/20
Epoch 51 | Train Loss: 1.5838 | Val Loss: 9.3761
No improvement. Early stopping patience 20/20
Early stopping triggered!
Training complete.
Model saved to fish_bout_transformer.pth
